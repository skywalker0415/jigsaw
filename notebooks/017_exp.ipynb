{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a0127b5",
   "metadata": {},
   "source": [
    "![](https://images.unsplash.com/photo-1602084551218-a28205125639?ixlib=rb-1.2.1&ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&auto=format&fit=crop&w=2070&q=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64e40fa",
   "metadata": {},
   "source": [
    "<div class = 'alert alert-block alert-info'\n",
    "     style = 'background-color:#4c1c84;\n",
    "              color:#eeebf1;\n",
    "              border-width:5px;\n",
    "              border-color:#4c1c84;\n",
    "              font-family:Comic Sans MS;\n",
    "              border-radius: 50px 50px'>\n",
    "    <p style = 'font-size:24px'>Exp 017</p>\n",
    "    <a href = \"#Config\"\n",
    "       style = \"color:#eeebf1;\n",
    "                font-size:14px\">1.Config</a><br>\n",
    "    <a href = \"#Settings\"\n",
    "       style = \"color:#eeebf1;\n",
    "                font-size:14px\">2.Settings</a><br>\n",
    "    <a href = \"#Data-Load\"\n",
    "       style = \"color:#eeebf1;\n",
    "                font-size:14px\">3.Data Load</a><br>\n",
    "    <a href = \"#Pytorch-Settings\"\n",
    "       style = \"color:#eeebf1;\n",
    "                font-size:14px\">4.Pytorch Settings</a><br>\n",
    "    <a href = \"#Training\"\n",
    "       style = \"color:#eeebf1;\n",
    "                font-size:14px\">5.Training</a><br>\n",
    "</div>\n",
    "\n",
    "<p style = 'font-size:24px;\n",
    "            color:#4c1c84'>\n",
    "    実施したこと\n",
    "</p>\n",
    "    <li style = \"color:#4c1c84;\n",
    "                font-size:14px\">過去コンペの予測ラベル</li>\n",
    "    <li style = \"color:#4c1c84;\n",
    "                font-size:14px\">TFIDF</li>\n",
    "    <li style = \"color:#4c1c84;\n",
    "                font-size:14px\">SVR</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2617f8",
   "metadata": {},
   "source": [
    "<br>\n",
    "<h1 style = \"font-size:45px; font-family:Comic Sans MS ; font-weight : normal; background-color: #4c1c84 ; color : #eeebf1; text-align: center; border-radius: 100px 100px;\">\n",
    "    Config\n",
    "</h1>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "248e7680",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src/utils/iterative-stratification/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e800704",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-19 03:58:21.881428: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .time    { background: #40CC40; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tbody td { text-align: left; }\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .sp {  opacity: 0.25;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "import os\n",
    "import gc\n",
    "gc.enable()\n",
    "import sys\n",
    "import glob\n",
    "import copy\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import string\n",
    "import psutil\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "from contextlib import contextmanager\n",
    "from collections import defaultdict\n",
    "from box import Box\n",
    "from typing import Optional\n",
    "from pprint import pprint\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import japanize_matplotlib\n",
    "\n",
    "from tqdm.auto import tqdm as tqdmp\n",
    "from tqdm.autonotebook import tqdm as tqdm\n",
    "tqdmp.pandas()\n",
    "\n",
    "## Model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel, AdamW\n",
    "from transformers import RobertaModel, RobertaForSequenceClassification\n",
    "from transformers import BertForSequenceClassification\n",
    "from transformers import BertTokenizer\n",
    "from transformers import RobertaTokenizer\n",
    "\n",
    "# Pytorch Lightning\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.utilities.seed import seed_everything\n",
    "from pytorch_lightning import callbacks\n",
    "from pytorch_lightning.callbacks.progress import ProgressBarBase\n",
    "from pytorch_lightning import LightningDataModule, LightningDataModule\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.loggers.csv_logs import CSVLogger\n",
    "from pytorch_lightning.callbacks import RichProgressBar\n",
    "\n",
    "# Model\n",
    "import lightgbm as lgb\n",
    "from sklearn.svm import SVC\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "import cuml\n",
    "from cuml.svm import SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8d9be29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "config = {\n",
    "    \"seed\": 42,\n",
    "    \"root\": \"/content/drive/MyDrive/kaggle/Jigsaw/raw\",\n",
    "    \"n_fold\": 5,\n",
    "    \"epoch\": 3,\n",
    "    \"max_length\": 256,\n",
    "    \"environment\": \"AWS\",\n",
    "    \"project\": \"Jigsaw\",\n",
    "    \"entity\": \"dataskywalker\",\n",
    "    \"exp_name\": \"017_exp\",\n",
    "    \"margin\": 0.5,\n",
    "    \"train_fold\": [0, 1, 2, 3, 4],\n",
    "\n",
    "    \"trainer\": {\n",
    "        \"gpus\": 1,\n",
    "        \"accumulate_grad_batches\": 64,\n",
    "        \"progress_bar_refresh_rate\": 1,\n",
    "        \"fast_dev_run\": False,\n",
    "        \"num_sanity_val_steps\": 0,\n",
    "    },\n",
    "\n",
    "    \"train_loader\": {\n",
    "        \"batch_size\": 16,\n",
    "        \"shuffle\": False,\n",
    "        \"num_workers\": 4,\n",
    "        \"pin_memory\": True,\n",
    "        \"drop_last\": False,\n",
    "    },\n",
    "\n",
    "    \"valid_loader\": {\n",
    "        \"batch_size\": 4,\n",
    "        \"shuffle\": False,\n",
    "        \"num_workers\": 1,\n",
    "        \"pin_memory\": True,\n",
    "        \"drop_last\": False,\n",
    "    },\n",
    "\n",
    "    \"test_loader\": {\n",
    "        \"batch_size\": 8,\n",
    "        \"shuffle\": False,\n",
    "        \"num_workers\": 1,\n",
    "        \"pin_memory\": True,\n",
    "        \"drop_last\": False,\n",
    "    },\n",
    "\n",
    "    \"backbone\": {\n",
    "        \"name\": \"roberta-base\",\n",
    "        \"output_dim\": 1,\n",
    "    },\n",
    "\n",
    "    \"optimizer\": {\n",
    "        \"name\": \"torch.optim.AdamW\",\n",
    "        \"params\": {\n",
    "            \"lr\": 1e-5,\n",
    "        },\n",
    "    },\n",
    "\n",
    "    \"scheduler\": {\n",
    "        \"name\": \"torch.optim.lr_scheduler.CosineAnnealingWarmRestarts\",\n",
    "        \"params\": {\n",
    "            \"T_0\": 20,\n",
    "            \"eta_min\": 0,\n",
    "        },\n",
    "    },\n",
    "\n",
    "    \"loss\": \"nn.BCEWithLogitsLoss\",\n",
    "}\n",
    "\n",
    "config = Box(config)\n",
    "config.tokenizer = AutoTokenizer.from_pretrained(config.backbone.name)\n",
    "config.model = RobertaModel.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f31f79fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your environment is 'AWS'.\n",
      "INPUT_DIR is /mnt/work/data/kaggle/Jigsaw\n",
      "MODEL_DIR is ../models/017_exp\n",
      "OUTPUT_DIR is ../data/interim/017_exp\n",
      "UTIL_DIR is /mnt/work/shimizu/kaggle/PetFinder/src/utils\n"
     ]
    }
   ],
   "source": [
    "# 個人的にAWSやKaggle環境やGoogle Colabを行ったり来たりしているのでまとめています\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "if config.environment == 'AWS':\n",
    "    \n",
    "    INPUT_DIR = Path('/mnt/work/data/kaggle/Jigsaw/')\n",
    "    MODEL_DIR = Path(f'../models/{config.exp_name}/')\n",
    "    OUTPUT_DIR = Path(f'../data/interim/{config.exp_name}/')\n",
    "    UTIL_DIR = Path('/mnt/work/shimizu/kaggle/PetFinder/src/utils')\n",
    "    \n",
    "    os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    \n",
    "    print(f\"Your environment is 'AWS'.\\nINPUT_DIR is {INPUT_DIR}\\nMODEL_DIR is {MODEL_DIR}\\nOUTPUT_DIR is {OUTPUT_DIR}\\nUTIL_DIR is {UTIL_DIR}\")\n",
    "    \n",
    "    \n",
    "elif config.environment == 'Kaggle':\n",
    "    INPUT_DIR = Path('../input/*****')\n",
    "    MODEL_DIR = Path('./')\n",
    "    OUTPUT_DIR = Path('./')\n",
    "    print(f\"Your environment is 'Kaggle'.\\nINPUT_DIR is {INPUT_DIR}\\nMODEL_DIR is {MODEL_DIR}\\nOUTPUT_DIR is {OUTPUT_DIR}\")\n",
    "\n",
    "    \n",
    "elif config.environment == 'Colab':\n",
    "    INPUT_DIR = Path('/content/drive/MyDrive/kaggle/Jigsaw/raw')\n",
    "    BASE_DIR = Path(\"/content/drive/MyDrive/kaggle/Jigsaw/interim\")\n",
    "\n",
    "    MODEL_DIR = BASE_DIR / f'{config.exp_name}'\n",
    "    OUTPUT_DIR = BASE_DIR / f'{config.exp_name}/'\n",
    "\n",
    "    os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    \n",
    "    if not os.path.exists(INPUT_DIR):\n",
    "        print('Please Mount your Google Drive.')\n",
    "    else:\n",
    "        print(f\"Your environment is 'Colab'.\\nINPUT_DIR is {INPUT_DIR}\\nMODEL_DIR is {MODEL_DIR}\\nOUTPUT_DIR is {OUTPUT_DIR}\")\n",
    "        \n",
    "else:\n",
    "    print(\"Please choose 'AWS' or 'Kaggle' or 'Colab'.\\nINPUT_DIR is not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "266cdc61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Seed固定\n",
    "seed_everything(config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2048fe59",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 処理時間計測\n",
    "@contextmanager\n",
    "def timer(name:str, slack:bool=False):\n",
    "    t0 = time.time()\n",
    "    p = psutil.Process(os.getpid())\n",
    "    m0 = p.memory_info()[0] / 2. ** 30\n",
    "    print(f'<< {name} >> Start')\n",
    "    yield\n",
    "    \n",
    "    m1 = p.memory_info()[0] / 2. ** 30\n",
    "    delta = m1 - m0\n",
    "    sign = '+' if delta >= 0 else '-'\n",
    "    delta = math.fabs(delta)\n",
    "    \n",
    "    print(f\"<< {name} >> {m1:.1f}GB({sign}{delta:.1f}GB):{time.time() - t0:.1f}sec\", file=sys.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b740e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "## Tokenizer & Model Save\n",
    "config.tokenizer.save_pretrained(OUTPUT_DIR/f\"{config.backbone.name}\")\n",
    "pretrain_model = RobertaModel.from_pretrained('roberta-base')\n",
    "pretrain_model.save_pretrained(OUTPUT_DIR/f\"{config.backbone.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2af06d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaModel(\n",
       "  (embeddings): RobertaEmbeddings(\n",
       "    (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "    (token_type_embeddings): Embedding(1, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): RobertaEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): RobertaPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrain_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029cfaed",
   "metadata": {},
   "source": [
    "<br>\n",
    "<h1 style = \"font-size:45px; font-family:Comic Sans MS ; font-weight : normal; background-color: #4c1c84 ; color : #eeebf1; text-align: center; border-radius: 100px 100px;\">\n",
    "    Data Load\n",
    "</h1>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d463e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/work/data/kaggle/Jigsaw/comments_to_score.csv\n",
      "/mnt/work/data/kaggle/Jigsaw/sample_submission.csv\n",
      "/mnt/work/data/kaggle/Jigsaw/validation_data.csv\n"
     ]
    }
   ],
   "source": [
    "## Data Check\n",
    "for dirnames, _, filenames in os.walk(INPUT_DIR):\n",
    "    \n",
    "    for filename in filenames:\n",
    "\n",
    "        print(f'{dirnames}/{filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c1dc1cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>worker</th>\n",
       "      <th>less_toxic</th>\n",
       "      <th>more_toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>313</td>\n",
       "      <td>This article sucks \\n\\nwoo woo wooooooo</td>\n",
       "      <td>WHAT!!!!!!!!?!?!!?!?!!?!?!?!?!!!!!!!!!!!!!!!!!...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>188</td>\n",
       "      <td>\"And yes, people should recognize that but the...</td>\n",
       "      <td>Daphne Guinness \\n\\nTop of the mornin' my fav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>82</td>\n",
       "      <td>Western Media?\\n\\nYup, because every crime in...</td>\n",
       "      <td>\"Atom you don't believe actual photos of mastu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>347</td>\n",
       "      <td>And you removed it! You numbskull! I don't car...</td>\n",
       "      <td>You seem to have sand in your vagina.\\n\\nMight...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>539</td>\n",
       "      <td>smelly vagina \\n\\nBluerasberry why don't you ...</td>\n",
       "      <td>hey \\n\\nway to support nazis, you racist</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   worker                                         less_toxic  \\\n",
       "0     313            This article sucks \\n\\nwoo woo wooooooo   \n",
       "1     188  \"And yes, people should recognize that but the...   \n",
       "2      82   Western Media?\\n\\nYup, because every crime in...   \n",
       "3     347  And you removed it! You numbskull! I don't car...   \n",
       "4     539   smelly vagina \\n\\nBluerasberry why don't you ...   \n",
       "\n",
       "                                          more_toxic  \n",
       "0  WHAT!!!!!!!!?!?!!?!?!!?!?!?!?!!!!!!!!!!!!!!!!!...  \n",
       "1   Daphne Guinness \\n\\nTop of the mornin' my fav...  \n",
       "2  \"Atom you don't believe actual photos of mastu...  \n",
       "3  You seem to have sand in your vagina.\\n\\nMight...  \n",
       "4           hey \\n\\nway to support nazis, you racist  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>114890</td>\n",
       "      <td>\"\\n \\n\\nGjalexei, you asked about whether ther...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>732895</td>\n",
       "      <td>Looks like be have an abuser , can you please ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1139051</td>\n",
       "      <td>I confess to having complete (and apparently b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1434512</td>\n",
       "      <td>\"\\n\\nFreud's ideas are certainly much discusse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2084821</td>\n",
       "      <td>It is not just you. This is a laundry list of ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   comment_id                                               text\n",
       "0      114890  \"\\n \\n\\nGjalexei, you asked about whether ther...\n",
       "1      732895  Looks like be have an abuser , can you please ...\n",
       "2     1139051  I confess to having complete (and apparently b...\n",
       "3     1434512  \"\\n\\nFreud's ideas are certainly much discusse...\n",
       "4     2084821  It is not just you. This is a laundry list of ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val_df = pd.read_csv(\"/mnt/work/data/kaggle/Jigsaw/validation_data.csv\")\n",
    "test_df = pd.read_csv(\"/mnt/work/data/kaggle/Jigsaw/comments_to_score.csv\")\n",
    "\n",
    "display(val_df.head())\n",
    "display(test_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225edb27",
   "metadata": {},
   "source": [
    "<br>\n",
    "<h2 style = \"font-size:45px; font-family:Comic Sans MS ; font-weight : normal; background-color: #4c1c84 ; color : #eeebf1; text-align: center; border-radius: 100px 100px;\">\n",
    "    Ruddit\n",
    "</h2>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6636882d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>txt</th>\n",
       "      <th>url</th>\n",
       "      <th>offensiveness_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>42g75o</td>\n",
       "      <td>cza1q49</td>\n",
       "      <td>&gt; The difference in average earnings between m...</td>\n",
       "      <td>https://www.reddit.com/r/changemyview/comments...</td>\n",
       "      <td>-0.083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>42g75o</td>\n",
       "      <td>cza1wdh</td>\n",
       "      <td>The myth is that the \"gap\" is entirely based o...</td>\n",
       "      <td>https://www.reddit.com/r/changemyview/comments...</td>\n",
       "      <td>-0.022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>42g75o</td>\n",
       "      <td>cza23qx</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>https://www.reddit.com/r/changemyview/comments...</td>\n",
       "      <td>0.167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>42g75o</td>\n",
       "      <td>cza2bw8</td>\n",
       "      <td>The assertion is that women get paid less for ...</td>\n",
       "      <td>https://www.reddit.com/r/changemyview/comments...</td>\n",
       "      <td>-0.146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>42g75o</td>\n",
       "      <td>cza2iji</td>\n",
       "      <td>You said in the OP that's not what they're mea...</td>\n",
       "      <td>https://www.reddit.com/r/changemyview/comments...</td>\n",
       "      <td>-0.083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5833</th>\n",
       "      <td>cu67co</td>\n",
       "      <td>f0i0mqp</td>\n",
       "      <td>They should only censor things that talk badly...</td>\n",
       "      <td>https://i.redd.it/kfsmqzxae3i31.jpg/f0i0mqp/</td>\n",
       "      <td>0.064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5834</th>\n",
       "      <td>cganu1</td>\n",
       "      <td>f80wlxq</td>\n",
       "      <td>&gt; and one of them is a woman. \\n\\nOH SHIT we b...</td>\n",
       "      <td>https://www.reddit.com/r/worldpolitics/comment...</td>\n",
       "      <td>0.458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5835</th>\n",
       "      <td>cu67co</td>\n",
       "      <td>f8uksbp</td>\n",
       "      <td>how is this flared as US politics</td>\n",
       "      <td>https://i.redd.it/kfsmqzxae3i31.jpg/f8uksbp/</td>\n",
       "      <td>-0.292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5836</th>\n",
       "      <td>cganu1</td>\n",
       "      <td>fa6nc1r</td>\n",
       "      <td>People in Hong Kong must decide if they are go...</td>\n",
       "      <td>https://www.reddit.com/r/worldpolitics/comment...</td>\n",
       "      <td>0.333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5837</th>\n",
       "      <td>cqqera</td>\n",
       "      <td>fakgh1h</td>\n",
       "      <td>I know this is an old post but I saw him last ...</td>\n",
       "      <td>https://i.redd.it/xt63a5xefmg31.jpg/fakgh1h/</td>\n",
       "      <td>-0.625</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5838 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     post_id comment_id                                                txt  \\\n",
       "0     42g75o    cza1q49  > The difference in average earnings between m...   \n",
       "1     42g75o    cza1wdh  The myth is that the \"gap\" is entirely based o...   \n",
       "2     42g75o    cza23qx                                          [deleted]   \n",
       "3     42g75o    cza2bw8  The assertion is that women get paid less for ...   \n",
       "4     42g75o    cza2iji  You said in the OP that's not what they're mea...   \n",
       "...      ...        ...                                                ...   \n",
       "5833  cu67co    f0i0mqp  They should only censor things that talk badly...   \n",
       "5834  cganu1    f80wlxq  > and one of them is a woman. \\n\\nOH SHIT we b...   \n",
       "5835  cu67co    f8uksbp                  how is this flared as US politics   \n",
       "5836  cganu1    fa6nc1r  People in Hong Kong must decide if they are go...   \n",
       "5837  cqqera    fakgh1h  I know this is an old post but I saw him last ...   \n",
       "\n",
       "                                                    url  offensiveness_score  \n",
       "0     https://www.reddit.com/r/changemyview/comments...               -0.083  \n",
       "1     https://www.reddit.com/r/changemyview/comments...               -0.022  \n",
       "2     https://www.reddit.com/r/changemyview/comments...                0.167  \n",
       "3     https://www.reddit.com/r/changemyview/comments...               -0.146  \n",
       "4     https://www.reddit.com/r/changemyview/comments...               -0.083  \n",
       "...                                                 ...                  ...  \n",
       "5833       https://i.redd.it/kfsmqzxae3i31.jpg/f0i0mqp/                0.064  \n",
       "5834  https://www.reddit.com/r/worldpolitics/comment...                0.458  \n",
       "5835       https://i.redd.it/kfsmqzxae3i31.jpg/f8uksbp/               -0.292  \n",
       "5836  https://www.reddit.com/r/worldpolitics/comment...                0.333  \n",
       "5837       https://i.redd.it/xt63a5xefmg31.jpg/fakgh1h/               -0.625  \n",
       "\n",
       "[5838 rows x 5 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"../data/external/Ruddit/Dataset/ruddit_with_text.csv\")\n",
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c87f41",
   "metadata": {},
   "source": [
    "\n",
    "<li style = \"color:#e60033;\n",
    "             font-size:20px\">\n",
    "    [deleted]は外した方がいい？\n",
    "</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8230c6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "sns.distplot(train_df[\"offensiveness_score\"])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b2b2b52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['y'] = (train_df['offensiveness_score'] - train_df.offensiveness_score.min()) / (train_df.offensiveness_score.max() - train_df.offensiveness_score.min()) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c9699a48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "sns.distplot(train_df[\"y\"])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "404f2ff0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>txt</th>\n",
       "      <th>url</th>\n",
       "      <th>offensiveness_score</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>42g75o</td>\n",
       "      <td>cza1q49</td>\n",
       "      <td>&gt; The difference in average earnings between m...</td>\n",
       "      <td>https://www.reddit.com/r/changemyview/comments...</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>0.431478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>42g75o</td>\n",
       "      <td>cza1wdh</td>\n",
       "      <td>The myth is that the \"gap\" is entirely based o...</td>\n",
       "      <td>https://www.reddit.com/r/changemyview/comments...</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>0.464133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>42g75o</td>\n",
       "      <td>cza2bw8</td>\n",
       "      <td>The assertion is that women get paid less for ...</td>\n",
       "      <td>https://www.reddit.com/r/changemyview/comments...</td>\n",
       "      <td>-0.146</td>\n",
       "      <td>0.397752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>42g75o</td>\n",
       "      <td>cza2iji</td>\n",
       "      <td>You said in the OP that's not what they're mea...</td>\n",
       "      <td>https://www.reddit.com/r/changemyview/comments...</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>0.431478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>42g75o</td>\n",
       "      <td>cza2jj3</td>\n",
       "      <td>&gt;Men and women are not payed less for the same...</td>\n",
       "      <td>https://www.reddit.com/r/changemyview/comments...</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>0.453426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5717</th>\n",
       "      <td>cu67co</td>\n",
       "      <td>f0i0mqp</td>\n",
       "      <td>They should only censor things that talk badly...</td>\n",
       "      <td>https://i.redd.it/kfsmqzxae3i31.jpg/f0i0mqp/</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.510171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5718</th>\n",
       "      <td>cganu1</td>\n",
       "      <td>f80wlxq</td>\n",
       "      <td>&gt; and one of them is a woman. \\n\\nOH SHIT we b...</td>\n",
       "      <td>https://www.reddit.com/r/worldpolitics/comment...</td>\n",
       "      <td>0.458</td>\n",
       "      <td>0.721092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5719</th>\n",
       "      <td>cu67co</td>\n",
       "      <td>f8uksbp</td>\n",
       "      <td>how is this flared as US politics</td>\n",
       "      <td>https://i.redd.it/kfsmqzxae3i31.jpg/f8uksbp/</td>\n",
       "      <td>-0.292</td>\n",
       "      <td>0.319593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5720</th>\n",
       "      <td>cganu1</td>\n",
       "      <td>fa6nc1r</td>\n",
       "      <td>People in Hong Kong must decide if they are go...</td>\n",
       "      <td>https://www.reddit.com/r/worldpolitics/comment...</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.654176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5721</th>\n",
       "      <td>cqqera</td>\n",
       "      <td>fakgh1h</td>\n",
       "      <td>I know this is an old post but I saw him last ...</td>\n",
       "      <td>https://i.redd.it/xt63a5xefmg31.jpg/fakgh1h/</td>\n",
       "      <td>-0.625</td>\n",
       "      <td>0.141328</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5722 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     post_id comment_id                                                txt  \\\n",
       "0     42g75o    cza1q49  > The difference in average earnings between m...   \n",
       "1     42g75o    cza1wdh  The myth is that the \"gap\" is entirely based o...   \n",
       "2     42g75o    cza2bw8  The assertion is that women get paid less for ...   \n",
       "3     42g75o    cza2iji  You said in the OP that's not what they're mea...   \n",
       "4     42g75o    cza2jj3  >Men and women are not payed less for the same...   \n",
       "...      ...        ...                                                ...   \n",
       "5717  cu67co    f0i0mqp  They should only censor things that talk badly...   \n",
       "5718  cganu1    f80wlxq  > and one of them is a woman. \\n\\nOH SHIT we b...   \n",
       "5719  cu67co    f8uksbp                  how is this flared as US politics   \n",
       "5720  cganu1    fa6nc1r  People in Hong Kong must decide if they are go...   \n",
       "5721  cqqera    fakgh1h  I know this is an old post but I saw him last ...   \n",
       "\n",
       "                                                    url  offensiveness_score  \\\n",
       "0     https://www.reddit.com/r/changemyview/comments...               -0.083   \n",
       "1     https://www.reddit.com/r/changemyview/comments...               -0.022   \n",
       "2     https://www.reddit.com/r/changemyview/comments...               -0.146   \n",
       "3     https://www.reddit.com/r/changemyview/comments...               -0.083   \n",
       "4     https://www.reddit.com/r/changemyview/comments...               -0.042   \n",
       "...                                                 ...                  ...   \n",
       "5717       https://i.redd.it/kfsmqzxae3i31.jpg/f0i0mqp/                0.064   \n",
       "5718  https://www.reddit.com/r/worldpolitics/comment...                0.458   \n",
       "5719       https://i.redd.it/kfsmqzxae3i31.jpg/f8uksbp/               -0.292   \n",
       "5720  https://www.reddit.com/r/worldpolitics/comment...                0.333   \n",
       "5721       https://i.redd.it/xt63a5xefmg31.jpg/fakgh1h/               -0.625   \n",
       "\n",
       "             y  \n",
       "0     0.431478  \n",
       "1     0.464133  \n",
       "2     0.397752  \n",
       "3     0.431478  \n",
       "4     0.453426  \n",
       "...        ...  \n",
       "5717  0.510171  \n",
       "5718  0.721092  \n",
       "5719  0.319593  \n",
       "5720  0.654176  \n",
       "5721  0.141328  \n",
       "\n",
       "[5722 rows x 6 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## [deleted]は除去\n",
    "train_df = train_df[train_df[\"txt\"]!=\"[deleted]\"].reset_index(drop=True)\n",
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5a24c7",
   "metadata": {},
   "source": [
    "<br>\n",
    "<h1 style = \"font-size:60px; font-family:Comic Sans MS ; font-weight : normal; background-color: #4c1c84 ; color : #eeebf1; text-align: center; border-radius: 100px 100px;\">\n",
    "    Make Dataset\n",
    "</h1>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f3123691",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JigsawDataset:\n",
    "\n",
    "    def __init__(self, df, max_length, tokenizer, mode):\n",
    "\n",
    "        self.df = df\n",
    "        self.max_len = max_length\n",
    "        self.tokenizer = tokenizer\n",
    "        self.mode = mode\n",
    "        \n",
    "        if self.mode == \"train\":\n",
    "\n",
    "            self.text = df[\"txt\"].values\n",
    "            self.target = df[\"y\"].values\n",
    "            \n",
    "        elif self.mode == \"valid\":\n",
    "            \n",
    "            self.more_toxic = df[\"more_toxic\"].values\n",
    "            self.less_toxic = df[\"less_toxic\"].values\n",
    "            \n",
    "        else:\n",
    "            self.text = df[\"text\"].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        if self.mode == \"train\":\n",
    "                \n",
    "            text = self.text[idx]\n",
    "            target = self.target[idx]\n",
    "\n",
    "            inputs_text = self.tokenizer.encode_plus(\n",
    "                text,\n",
    "                truncation=True,\n",
    "                return_attention_mask=True,\n",
    "                return_token_type_ids=True,\n",
    "                max_length = self.max_len,\n",
    "                padding=\"max_length\"\n",
    "            )\n",
    "\n",
    "\n",
    "            text_ids = inputs_text[\"input_ids\"]\n",
    "            text_mask = inputs_text[\"attention_mask\"]\n",
    "            text_token_type_ids = inputs_text[\"token_type_ids\"]\n",
    "\n",
    "            return {\n",
    "                'text_ids': torch.tensor(text_ids, dtype=torch.long),\n",
    "                'text_mask': torch.tensor(text_mask, dtype=torch.long),\n",
    "                'text_token_type_ids': torch.tensor(text_token_type_ids, dtype=torch.long),\n",
    "                'target': torch.tensor(target, dtype=torch.float)\n",
    "            }\n",
    "        \n",
    "        elif self.mode == \"valid\":\n",
    "            \n",
    "            more_toxic = self.more_toxic[idx]\n",
    "            less_toxic = self.less_toxic[idx]\n",
    "\n",
    "            inputs_more_toxic = self.tokenizer.encode_plus(\n",
    "                more_toxic,\n",
    "                truncation=True,\n",
    "                return_attention_mask=True,\n",
    "                return_token_type_ids=True,\n",
    "                max_length = self.max_len,\n",
    "                padding=\"max_length\"\n",
    "            )\n",
    "\n",
    "            inputs_less_toxic = self.tokenizer.encode_plus(\n",
    "                less_toxic,\n",
    "                truncation=True,\n",
    "                return_attention_mask=True,\n",
    "                return_token_type_ids=True,\n",
    "                max_length = self.max_len,\n",
    "                padding=\"max_length\"\n",
    "            )\n",
    "\n",
    "            target = 1\n",
    "\n",
    "            more_toxic_ids = inputs_more_toxic[\"input_ids\"]\n",
    "            more_toxic_mask = inputs_more_toxic[\"attention_mask\"]\n",
    "            more_toxic_token_type_ids = inputs_more_toxic[\"token_type_ids\"]\n",
    "\n",
    "            less_toxic_ids = inputs_less_toxic[\"input_ids\"]\n",
    "            less_toxic_mask = inputs_less_toxic[\"attention_mask\"]\n",
    "            less_toxic_token_type_ids = inputs_less_toxic[\"token_type_ids\"]\n",
    "\n",
    "            return {\n",
    "                'more_toxic_ids': torch.tensor(more_toxic_ids, dtype=torch.long),\n",
    "                'more_toxic_mask': torch.tensor(more_toxic_mask, dtype=torch.long),\n",
    "                'more_toxic_token_type_ids': torch.tensor(more_toxic_token_type_ids, dtype=torch.long),\n",
    "                'less_toxic_ids': torch.tensor(less_toxic_ids, dtype=torch.long),\n",
    "                'less_toxic_mask': torch.tensor(less_toxic_mask, dtype=torch.long),\n",
    "                'less_toxic_token_type_ids': torch.tensor(less_toxic_token_type_ids, dtype=torch.long),\n",
    "                'target': torch.tensor(target, dtype=torch.long)\n",
    "            }\n",
    "\n",
    "        else:\n",
    "            text = self.text[idx]\n",
    "\n",
    "            inputs_text = self.tokenizer.encode_plus(\n",
    "                text,\n",
    "                truncation=True,\n",
    "                return_attention_mask=True,\n",
    "                return_token_type_ids=True,\n",
    "                max_length = self.max_len,\n",
    "                padding=\"max_length\"\n",
    "            )\n",
    "\n",
    "\n",
    "            text_ids = inputs_text[\"input_ids\"]\n",
    "            text_mask = inputs_text[\"attention_mask\"]\n",
    "            text_token_type_ids = inputs_text[\"token_type_ids\"]\n",
    "\n",
    "            return {\n",
    "                'text_ids': torch.tensor(text_ids, dtype=torch.long),\n",
    "                'text_mask': torch.tensor(text_mask, dtype=torch.long),\n",
    "                'text_token_type_ids': torch.tensor(text_token_type_ids, dtype=torch.long),\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a22f67aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JigsawDataModule(LightningDataModule):\n",
    "\n",
    "    def __init__(self, train_df, valid_df, test_df, cfg):\n",
    "\n",
    "        super().__init__()\n",
    "        self._train_df = train_df\n",
    "        self._valid_df = valid_df\n",
    "        self._test_df = test_df\n",
    "        self._cfg = cfg\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        dataset = JigsawDataset(\n",
    "            df=self._train_df, \n",
    "            max_length=self._cfg.max_length,\n",
    "            tokenizer=self._cfg.tokenizer,\n",
    "            mode=\"train\"\n",
    "            )\n",
    "        return DataLoader(dataset, **self._cfg.train_loader)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        dataset = JigsawDataset(\n",
    "            df=self._valid_df, \n",
    "            max_length=self._cfg.max_length,\n",
    "            tokenizer=self._cfg.tokenizer,\n",
    "            mode=\"train\"\n",
    "            )\n",
    "        return DataLoader(dataset, **self._cfg.valid_loader)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        dataset = JigsawDataset(\n",
    "            df=self._test_df, \n",
    "            max_length=self._cfg.max_length,\n",
    "            tokenizer=self._cfg.tokenizer,\n",
    "            mode=\"test\"\n",
    "            )\n",
    "\n",
    "        return DataLoader(dataset, **self._cfg.test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "92ae6248",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Check\n",
    "\n",
    "sample_dataloader = JigsawDataModule(train_df, val_df, test_df, config).train_dataloader()\n",
    "for data in sample_dataloader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0568ed61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_ids size torch.Size([16, 256])\n",
      "text_mask size torch.Size([16, 256])\n",
      "text_token_type_ids size torch.Size([16, 256])\n",
      "target size torch.Size([16])\n",
      "target tensor([0.4315, 0.4641, 0.3978, 0.4315, 0.4534, 0.4647, 0.3828, 0.3533, 0.4759,\n",
      "        0.5284, 0.4315, 0.4427, 0.4427, 0.4647, 0.4315, 0.3753])\n",
      "Embedding size torch.Size([16, 256, 768])\n",
      "Attention size torch.Size([16, 12, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "print(\"text_ids size\", data[\"text_ids\"].size())\n",
    "print(\"text_mask size\", data[\"text_mask\"].size())\n",
    "print(\"text_token_type_ids size\", data[\"text_token_type_ids\"].size())\n",
    "print(\"target size\", data[\"target\"].size())\n",
    "print(\"target\", data[\"target\"])\n",
    "\n",
    "outputs = pretrain_model(\n",
    "    data[\"text_ids\"], \n",
    "    data[\"text_mask\"], \n",
    "    data[\"text_token_type_ids\"],\n",
    "    output_hidden_states=True,\n",
    "    output_attentions=True,\n",
    ")\n",
    "print(\"Embedding size\", outputs.last_hidden_state.size())\n",
    "print(\"Attention size\", outputs.attentions[-1].size())\n",
    "# print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "962af381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Data Check\n",
    "\n",
    "# sample_dataloader = JigsawDataModule(train_df, train_df, test_df, config).val_dataloader()\n",
    "# for data in sample_dataloader:\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c52ae8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"text_ids size\", data[\"text_ids\"].size())\n",
    "# print(\"text_mask size\", data[\"text_mask\"].size())\n",
    "# print(\"text_token_type_ids size\", data[\"text_token_type_ids\"].size())\n",
    "# print(\"target size\", data[\"target\"].size())\n",
    "# print(\"target\", data[\"target\"])\n",
    "# print(\"Embedding size\", pretrain_model(data[\"text_ids\"], data[\"text_mask\"], data[\"text_token_type_ids\"])[\"last_hidden_state\"].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "62106b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"text_ids size\", data[\"more_toxic_ids\"].size())\n",
    "# print(\"text_mask size\", data[\"more_toxic_mask\"].size())\n",
    "# print(\"text_token_type_ids size\", data[\"more_toxic_token_type_ids\"].size())\n",
    "# print(\"target size\", data[\"target\"].size())\n",
    "# print(\"target\", data[\"target\"])\n",
    "# print(\"Embedding size\", pretrain_model(\n",
    "#     data[\"more_toxic_ids\"], \n",
    "#     data[\"more_toxic_mask\"], \n",
    "#     data[\"more_toxic_token_type_ids\"])[\"last_hidden_state\"].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d2c5b2",
   "metadata": {},
   "source": [
    "<br>\n",
    "<h1 style = \"font-size:45px; font-family:Comic Sans MS ; font-weight : normal; background-color: #4c1c84 ; color : #eeebf1; text-align: center; border-radius: 100px 100px;\">\n",
    "    Model\n",
    "</h1>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "47706ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSELoss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RMSELoss,self).__init__()\n",
    "\n",
    "    def forward(self,x,y):\n",
    "        criterion = nn.MSELoss()\n",
    "        loss = torch.sqrt(criterion(x, y))\n",
    "        return loss\n",
    "\n",
    "def valid_criterion(outputs1, outputs2, targets):\n",
    "    return nn.MarginRankingLoss(margin=config.margin)(outputs1, outputs2, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4152f82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JigsawModel(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self, cfg, fold_num):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.__build_model()\n",
    "        self.train_criterion = nn.BCEWithLogitsLoss()\n",
    "        self.valid_criterion = valid_criterion\n",
    "        self.save_hyperparameters(cfg)\n",
    "        self.fold_num = fold_num\n",
    "        \n",
    "    def __build_model(self):\n",
    "        \n",
    "        self.base_model = RobertaModel.from_pretrained('roberta-base')\n",
    "#         self.base_model = RobertaForSequenceClassification.from_pretrained(self.cfg.backbone.name)\n",
    "        print(f\"Use Model: {self.cfg.backbone.name}\")\n",
    "        \n",
    "        self.layer_norm = nn.LayerNorm(768)\n",
    "        self.drop = nn.Dropout(p=0.2)\n",
    "        self.head = nn.Sequential(nn.Linear(768, 256),\n",
    "                                  nn.LeakyReLU(negative_slope=0.01),\n",
    "                                  nn.Dropout(0.2),\n",
    "                                  nn.Linear(256, 1))\n",
    "        \n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        \n",
    "        out = self.base_model(\n",
    "            data[\"text_ids\"], \n",
    "            data[\"text_mask\"], \n",
    "            data[\"text_token_type_ids\"],\n",
    "            output_hidden_states=True,\n",
    "            output_attentions=True,\n",
    "        )\n",
    "#         out = self.base_model(input_ids=ids, attention_mask=mask, token_type_ids=token_type_ids)[\"last_hidden_state\"]\n",
    "        out = torch.mean(out.last_hidden_state ,dim=1)\n",
    "        out = self.layer_norm(out)\n",
    "        out = self.drop(out)\n",
    "        outputs = self.head(out)\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \n",
    "        text_ids = batch['text_ids']\n",
    "        text_mask = batch['text_mask']\n",
    "        text_token_type_ids = batch['text_token_type_ids']\n",
    "        targets = batch['target']\n",
    "        \n",
    "        outputs = self.forward(text_ids, text_mask, text_token_type_ids)\n",
    "        eps = 1e-6\n",
    "        loss = torch.sqrt(self.train_criterion(outputs, targets) + eps)\n",
    "\n",
    "        return {\"loss\":loss, \"targets\":targets}\n",
    "    \n",
    "    def training_epoch_end(self, training_step_outputs):\n",
    "        \n",
    "        loss_list = []\n",
    "        target_list = []\n",
    "        \n",
    "        for out in training_step_outputs:\n",
    "            \n",
    "            loss_list.extend([out[\"loss\"].cpu().detach().tolist()])\n",
    "            target_list.extend(out[\"targets\"].cpu().detach().numpy())\n",
    "            \n",
    "        meanloss = sum(loss_list)/len(loss_list)\n",
    "        logs = {f\"train_loss/fold{self.fold_num+1}\": meanloss,}\n",
    "        \n",
    "        self.log_dict(logs, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        \n",
    "        text_ids = batch['text_ids']\n",
    "        text_mask = batch['text_mask']\n",
    "        text_token_type_ids = batch['text_token_type_ids']\n",
    "        targets = batch['target']\n",
    "        \n",
    "        outputs = self.forward(text_ids, text_mask, text_token_type_ids)\n",
    "        eps = 1e-6\n",
    "        loss = torch.sqrt(self.train_criterion(outputs, targets) + eps)\n",
    "\n",
    "        return {\"loss\":loss, \"pred\":outputs, \"targets\":targets}\n",
    "    \n",
    "    def validation_epoch_end(self, validation_step_outputs):\n",
    "\n",
    "        loss_list = []\n",
    "        pred_list = []\n",
    "        target_list = []\n",
    "\n",
    "        for out in validation_step_outputs:\n",
    "            loss_list.extend([out[\"loss\"].cpu().detach().tolist()])\n",
    "            pred_list.append(out[\"pred\"].cpu().detach().numpy())\n",
    "            target_list.append(out[\"targets\"].cpu().detach().numpy())\n",
    "\n",
    "        meanloss = sum(loss_list)/len(loss_list)\n",
    "        pred_list = np.concatenate(pred_list)\n",
    "        target_list = np.concatenate(target_list)\n",
    "        \n",
    "        plt.figure(figsize=(12, 4))\n",
    "        sns.distplot(pred_list, label=\"pred\")\n",
    "        sns.distplot(target_list, label=\"pred\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "#         acc = np.sum(pred_list > 0)/len(pred_list)\n",
    "#         acc = sum([i > 0 for i in pred_list])/len(pred_list)\n",
    "\n",
    "        logs = {\n",
    "            f\"valid_loss/fold{self.fold_num+1}\":meanloss,\n",
    "#             f\"valid_acc/fold{self.fold_num+1}\":acc,\n",
    "        }\n",
    "\n",
    "        self.log_dict(logs, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        \n",
    "    def test_step(self, batch, batch_idx):\n",
    "\n",
    "        text_ids = batch['ids']\n",
    "        text_mask = batch['mask']\n",
    "\n",
    "        pred = self.forward(text_ids, text_mask)\n",
    "        return {\"pred\":pred,}\n",
    "    \n",
    "    def test_epoch_end(self, test_step_outputs):\n",
    "\n",
    "        pred_list = []\n",
    "\n",
    "        for out in test_step_outputs:\n",
    "            pred_list.extend([out[\"pred\"].cpu().detach().numpy()])\n",
    "\n",
    "        logs = {f\"pred\":pred_list,}\n",
    "\n",
    "        self.log_dict(logs, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "\n",
    "        optimizer = eval(self.cfg.optimizer.name)(self.parameters(), **self.cfg.optimizer.params)\n",
    "\n",
    "        self.scheduler = eval(self.cfg.scheduler.name)(optimizer, **self.cfg.scheduler.params)\n",
    "        \n",
    "        scheduler = {\"scheduler\": self.scheduler, \"interval\": \"step\",}\n",
    "\n",
    "        return [optimizer], [scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5a330e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class JigsawModel(pl.LightningModule):\n",
    "    \n",
    "#     def __init__(self, cfg, fold_num):\n",
    "# #         \n",
    "#         super().__init__()\n",
    "#         self.cfg = cfg\n",
    "#         self.__build_model()\n",
    "#         self.train_criterion = RMSELoss()\n",
    "#         self.valid_criterion = valid_criterion\n",
    "#         self.save_hyperparameters(cfg)\n",
    "#         self.fold_num = fold_num\n",
    "        \n",
    "#     def __build_model(self):\n",
    "        \n",
    "#         self.base_model = AutoModel.from_pretrained(self.cfg.backbone.name)\n",
    "#         print(f\"Use Model: {self.cfg.backbone.name}\")\n",
    "        \n",
    "#         self.layer_norm = nn.LayerNorm(768)\n",
    "#         self.drop = nn.Dropout(p=0.2)\n",
    "#         self.head = nn.Sequential(nn.Linear(768, 256),\n",
    "#                                   nn.LeakyReLU(negative_slope=0.01),\n",
    "#                                   nn.Dropout(0.2),\n",
    "#                                   nn.Linear(256, 1))\n",
    "        \n",
    "#     def forward(self, ids, mask, token_type_ids):\n",
    "        \n",
    "#         out = self.base_model(input_ids=ids, attention_mask=mask, token_type_ids=token_type_ids, output_hidden_states=False)\n",
    "#         out = self.layer_norm(out[1])\n",
    "#         out = self.drop(out)\n",
    "#         outputs = self.head(out)\n",
    "        \n",
    "#         return outputs\n",
    "    \n",
    "#     def training_step(self, batch, batch_idx):\n",
    "        \n",
    "#         text_ids = batch['text_ids']\n",
    "#         text_mask = batch['text_mask']\n",
    "#         text_token_type_ids = batch['text_token_type_ids']\n",
    "#         targets = batch['target']\n",
    "        \n",
    "#         outputs = self.forward(text_ids, text_mask, text_token_type_ids)[:, 0]\n",
    "#         eps = 1e-6\n",
    "#         loss = torch.sqrt(self.train_criterion(outputs, targets) + eps)\n",
    "\n",
    "#         return {\"loss\":loss, \"targets\":targets}\n",
    "    \n",
    "#     def training_epoch_end(self, training_step_outputs):\n",
    "        \n",
    "#         loss_list = []\n",
    "#         target_list = []\n",
    "        \n",
    "#         for out in training_step_outputs:\n",
    "            \n",
    "#             loss_list.extend([out[\"loss\"].cpu().detach().tolist()])\n",
    "#             target_list.extend(out[\"targets\"])\n",
    "            \n",
    "#         meanloss = sum(loss_list)/len(loss_list)\n",
    "#         logs = {f\"train_loss/fold{self.fold_num+1}\": meanloss,}\n",
    "        \n",
    "#         self.log_dict(logs, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        \n",
    "#     def validation_step(self, batch, batch_idx):\n",
    "        \n",
    "#         more_toxic_ids = batch['more_toxic_ids']\n",
    "#         more_toxic_mask = batch['more_toxic_mask']\n",
    "#         more_toxic_token_type_ids = batch['more_toxic_token_type_ids']\n",
    "#         less_toxic_ids = batch['less_toxic_ids']\n",
    "#         less_toxic_mask = batch['less_toxic_mask']\n",
    "#         less_token_type_ids = batch['less_toxic_token_type_ids']\n",
    "#         targets = batch['target']\n",
    "        \n",
    "#         more_toxic_outputs = self.forward(\n",
    "#             more_toxic_ids,\n",
    "#             more_toxic_mask, \n",
    "#             more_toxic_token_type_ids\n",
    "#         )\n",
    "#         less_toxic_outputs = self.forward(\n",
    "#             less_toxic_ids, \n",
    "#             less_toxic_mask, \n",
    "#             less_token_type_ids\n",
    "#         )\n",
    "        \n",
    "#         pred = more_toxic_outputs - less_toxic_outputs\n",
    "#         loss = self.valid_criterion(more_toxic_outputs, less_toxic_outputs, targets)\n",
    "\n",
    "#         return {\"loss\":loss, \"pred\":pred, \"targets\":targets}\n",
    "    \n",
    "#     def validation_epoch_end(self, validation_step_outputs):\n",
    "\n",
    "#         loss_list = []\n",
    "#         pred_list = []\n",
    "#         target_list = []\n",
    "\n",
    "#         for out in validation_step_outputs:\n",
    "#             loss_list.extend([out[\"loss\"].cpu().detach().tolist()])\n",
    "#             pred_list.append(out[\"pred\"].cpu().detach().numpy())\n",
    "#             target_list.extend(out[\"targets\"])\n",
    "\n",
    "#         meanloss = sum(loss_list)/len(loss_list)\n",
    "#         pred_list = np.concatenate(pred_list)\n",
    "#         acc = np.sum(pred_list > 0)/len(pred_list)\n",
    "# #         acc = sum([i > 0 for i in pred_list])/len(pred_list)\n",
    "\n",
    "#         logs = {\n",
    "#             f\"valid_loss/fold{self.fold_num+1}\":meanloss,\n",
    "#             f\"valid_acc/fold{self.fold_num+1}\":acc,\n",
    "#         }\n",
    "\n",
    "#         self.log_dict(logs, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        \n",
    "#     def test_step(self, batch, batch_idx):\n",
    "\n",
    "#         text_ids = batch['ids']\n",
    "#         text_mask = batch['mask']\n",
    "\n",
    "#         pred = self.forward(text_ids, text_mask)\n",
    "#         return {\"pred\":pred,}\n",
    "    \n",
    "#     def test_epoch_end(self, test_step_outputs):\n",
    "\n",
    "#         pred_list = []\n",
    "\n",
    "#         for out in test_step_outputs:\n",
    "#             pred_list.extend([out[\"pred\"].cpu().detach().numpy()])\n",
    "\n",
    "#         logs = {f\"pred\":pred_list,}\n",
    "\n",
    "#         self.log_dict(logs, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        \n",
    "#     def configure_optimizers(self):\n",
    "\n",
    "#         optimizer = eval(self.cfg.optimizer.name)(self.parameters(), **self.cfg.optimizer.params)\n",
    "\n",
    "#         self.scheduler = eval(self.cfg.scheduler.name)(optimizer, **self.cfg.scheduler.params)\n",
    "        \n",
    "#         scheduler = {\"scheduler\": self.scheduler, \"interval\": \"step\",}\n",
    "\n",
    "#         return [optimizer], [scheduler]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db04203",
   "metadata": {},
   "source": [
    "<br>\n",
    "<h1 style = \"font-size:45px; font-family:Comic Sans MS ; font-weight : normal; background-color: #4c1c84 ; color : #eeebf1; text-align: center; border-radius: 100px 100px;\">\n",
    "    Training\n",
    "</h1>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6f386141",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>txt</th>\n",
       "      <th>url</th>\n",
       "      <th>offensiveness_score</th>\n",
       "      <th>y</th>\n",
       "      <th>kfold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>42g75o</td>\n",
       "      <td>cza1q49</td>\n",
       "      <td>&gt; The difference in average earnings between m...</td>\n",
       "      <td>https://www.reddit.com/r/changemyview/comments...</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>0.431478</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>42g75o</td>\n",
       "      <td>cza1wdh</td>\n",
       "      <td>The myth is that the \"gap\" is entirely based o...</td>\n",
       "      <td>https://www.reddit.com/r/changemyview/comments...</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>0.464133</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>42g75o</td>\n",
       "      <td>cza2bw8</td>\n",
       "      <td>The assertion is that women get paid less for ...</td>\n",
       "      <td>https://www.reddit.com/r/changemyview/comments...</td>\n",
       "      <td>-0.146</td>\n",
       "      <td>0.397752</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>42g75o</td>\n",
       "      <td>cza2iji</td>\n",
       "      <td>You said in the OP that's not what they're mea...</td>\n",
       "      <td>https://www.reddit.com/r/changemyview/comments...</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>0.431478</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>42g75o</td>\n",
       "      <td>cza2jj3</td>\n",
       "      <td>&gt;Men and women are not payed less for the same...</td>\n",
       "      <td>https://www.reddit.com/r/changemyview/comments...</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>0.453426</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  post_id comment_id                                                txt  \\\n",
       "0  42g75o    cza1q49  > The difference in average earnings between m...   \n",
       "1  42g75o    cza1wdh  The myth is that the \"gap\" is entirely based o...   \n",
       "2  42g75o    cza2bw8  The assertion is that women get paid less for ...   \n",
       "3  42g75o    cza2iji  You said in the OP that's not what they're mea...   \n",
       "4  42g75o    cza2jj3  >Men and women are not payed less for the same...   \n",
       "\n",
       "                                                 url  offensiveness_score  \\\n",
       "0  https://www.reddit.com/r/changemyview/comments...               -0.083   \n",
       "1  https://www.reddit.com/r/changemyview/comments...               -0.022   \n",
       "2  https://www.reddit.com/r/changemyview/comments...               -0.146   \n",
       "3  https://www.reddit.com/r/changemyview/comments...               -0.083   \n",
       "4  https://www.reddit.com/r/changemyview/comments...               -0.042   \n",
       "\n",
       "          y  kfold  \n",
       "0  0.431478      2  \n",
       "1  0.464133      3  \n",
       "2  0.397752      3  \n",
       "3  0.431478      4  \n",
       "4  0.453426      4  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kf = KFold(n_splits=config.n_fold, shuffle=True, random_state=config.seed)\n",
    "\n",
    "for fold, (_, val_idx) in enumerate(kf.split(X=train_df, y=train_df[\"offensiveness_score\"])):\n",
    "    train_df.loc[val_idx, \"kfold\"] = int(fold)\n",
    "\n",
    "train_df[\"kfold\"] = train_df[\"kfold\"].astype(int)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "65defee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "★★★★★★★★★★★★★★★★★★★★★★★★★  Fold1  ★★★★★★★★★★★★★★★★★★★★★★★★★\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:91: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=1)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "  f\"Setting `Trainer(progress_bar_refresh_rate={progress_bar_refresh_rate})` is deprecated in v1.5 and\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use Model: roberta-base\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name            </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type              </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ base_model      │ RobertaModel      │  124 M │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>│ layer_norm      │ LayerNorm         │  1.5 K │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>│ drop            │ Dropout           │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span>│ head            │ Sequential        │  197 K │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 4 </span>│ train_criterion │ BCEWithLogitsLoss │      0 │\n",
       "└───┴─────────────────┴───────────────────┴────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName           \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType             \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ base_model      │ RobertaModel      │  124 M │\n",
       "│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ layer_norm      │ LayerNorm         │  1.5 K │\n",
       "│\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m│ drop            │ Dropout           │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m│ head            │ Sequential        │  197 K │\n",
       "│\u001b[2m \u001b[0m\u001b[2m4\u001b[0m\u001b[2m \u001b[0m│ train_criterion │ BCEWithLogitsLoss │      0 │\n",
       "└───┴─────────────────┴───────────────────┴────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 124 M                                                                      \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                      \n",
       "<span style=\"font-weight: bold\">Total params</span>: 124 M                                                                          \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 499                                                  \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 124 M                                                                      \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                      \n",
       "\u001b[1mTotal params\u001b[0m: 124 M                                                                          \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 499                                                  \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4b1247867724e3da0fbf45faf8e7f36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Epoch 0   </span> <span style=\"color: #3a3a3a; text-decoration-color: #3a3a3a\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">0/574</span> <span style=\"color: #8a8a8a; text-decoration-color: #8a8a8a\">0:00:01 • -:--:--</span> <span style=\"color: #b2b2b2; text-decoration-color: #b2b2b2\">0.00it/s</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[37mEpoch 0   \u001b[0m \u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[37m0/574\u001b[0m \u001b[38;5;245m0:00:01 • -:--:--\u001b[0m \u001b[38;5;249m0.00it/s\u001b[0m \u001b[37m \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Input, output and indices must be on the current device",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_9492/962017885.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     )\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdatamodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, train_dataloader, ckpt_path)\u001b[0m\n\u001b[1;32m    736\u001b[0m             \u001b[0mtrain_dataloaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    737\u001b[0m         self._call_and_handle_interrupt(\n\u001b[0;32m--> 738\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_impl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    739\u001b[0m         )\n\u001b[1;32m    740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(self, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    680\u001b[0m         \"\"\"\n\u001b[1;32m    681\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 682\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    683\u001b[0m         \u001b[0;31m# TODO: treat KeyboardInterrupt as BaseException (delete the code below) in v1.7\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    770\u001b[0m         \u001b[0;31m# TODO: ckpt_path only in v1.7\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    771\u001b[0m         \u001b[0mckpt_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mckpt_path\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 772\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    773\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    774\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1194\u001b[0m         \u001b[0;31m# dispatch `start_training` or `start_evaluating` or `start_predicting`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1195\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m         \u001b[0;31m# plugin will finalized fitting (e.g. ddp_spawn will load trained model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1273\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_type_plugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_predicting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1275\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_type_plugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_stage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\u001b[0m in \u001b[0;36mstart_training\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstart_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"pl.Trainer\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0;31m# double dispatch to initiate the training loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_stage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstart_evaluating\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"pl.Trainer\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mrun_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1283\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredicting\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1285\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_pre_training_routine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1313\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_detect_anomaly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_detect_anomaly\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1315\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1317\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0m_EVALUATE_OUTPUT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pytorch_lightning/loops/base.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madvance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestarting\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pytorch_lightning/loops/fit_loop.py\u001b[0m in \u001b[0;36madvance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run_training_epoch\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_fetcher\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m             \u001b[0;31m# the global step is manually decreased here due to backwards compatibility with existing loggers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pytorch_lightning/loops/base.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madvance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestarting\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py\u001b[0m in \u001b[0;36madvance\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run_training_batch\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m                 \u001b[0mbatch_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_progress\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mincrement_processed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pytorch_lightning/loops/base.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madvance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestarting\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py\u001b[0m in \u001b[0;36madvance\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautomatic_optimization\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0moptimizers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_active_optimizers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer_frequencies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pytorch_lightning/loops/base.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madvance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestarting\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\u001b[0m in \u001b[0;36madvance\u001b[0;34m(self, batch, *args, **kwargs)\u001b[0m\n\u001b[1;32m    217\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_idx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim_progress\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer_position\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer_idx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m         )\n\u001b[1;32m    221\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\u001b[0m in \u001b[0;36m_run_optimization\u001b[0;34m(self, split_batch, batch_idx, optimizer, opt_idx)\u001b[0m\n\u001b[1;32m    257\u001b[0m             \u001b[0;31m# automatic_optimization=True: perform ddp sync only when performing optimizer_step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_block_parallel_sync_behavior\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m                 \u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;31m# ------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\u001b[0m in \u001b[0;36mclosure\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mClosureResult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_profiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"training_step_and_backward\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0mstep_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstep_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclosure_loss\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\u001b[0m in \u001b[0;36m_training_step\u001b[0;34m(self, split_batch, batch_idx, opt_idx)\u001b[0m\n\u001b[1;32m    433\u001b[0m             \u001b[0mlightning_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_fx_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"training_step\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"training_step\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m                 \u001b[0mtraining_step_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_type_plugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_training_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pytorch_lightning/accelerators/accelerator.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, step_kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m         \"\"\"\n\u001b[1;32m    215\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecision_plugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_type_plugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mstep_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpost_training_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpost_training_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_9492/3001744504.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_token_type_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0meps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_criterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_9492/3001744504.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, ids, mask, token_type_ids)\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text_token_type_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         )\n\u001b[1;32m     35\u001b[0m \u001b[0;31m#         out = self.base_model(input_ids=ids, attention_mask=mask, token_type_ids=token_type_ids)[\"last_hidden_state\"]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    831\u001b[0m             \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m             \u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m             \u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m         )\n\u001b[1;32m    835\u001b[0m         encoder_outputs = self.encoder(\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m             \u001b[0minputs_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m         \u001b[0mtoken_type_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_type_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m         return F.embedding(\n\u001b[1;32m    125\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   1850\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1851\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1852\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1853\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1854\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input, output and indices must be on the current device"
     ]
    }
   ],
   "source": [
    "for fold in config.train_fold:\n",
    "    \n",
    "    print(\"★\"*25, f\" Fold{fold+1} \", \"★\"*25)\n",
    "\n",
    "    df_train = train_df[train_df.kfold != fold].reset_index(drop=True)\n",
    "    df_valid = train_df[train_df.kfold == fold].reset_index(drop=True)\n",
    "    \n",
    "    datamodule = JigsawDataModule(df_train, df_valid, test_df, config)\n",
    "    sample_dataloader = JigsawDataModule(df_train, df_valid, test_df, config).train_dataloader()\n",
    "\n",
    "    config.scheduler.params.T_0 = config.epoch * len(sample_dataloader)\n",
    "    model = JigsawModel(config, fold)\n",
    "    lr_monitor = callbacks.LearningRateMonitor()\n",
    "\n",
    "    loss_checkpoint = callbacks.ModelCheckpoint(\n",
    "        filename=f\"best_loss_fold{fold+1}\",\n",
    "        monitor=f\"valid_loss/fold{fold+1}\",\n",
    "        save_top_k=1,\n",
    "        mode=\"min\",\n",
    "        save_last=False,\n",
    "        dirpath=MODEL_DIR,\n",
    "    )\n",
    "\n",
    "    wandb_logger = WandbLogger(\n",
    "        project=config.project, \n",
    "        entity=config.entity,\n",
    "        name = f\"{config.exp_name}\",\n",
    "        tags = ['RoBERTa-Base', \"Ruddit\"]\n",
    "    )\n",
    "\n",
    "    lr_monitor = LearningRateMonitor(logging_interval='step')\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=config.epoch,\n",
    "        callbacks=[loss_checkpoint, lr_monitor, RichProgressBar()],\n",
    "        deterministic=True,\n",
    "        logger=[wandb_logger],\n",
    "        **config.trainer,\n",
    "    )\n",
    "    trainer.fit(model, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ae500e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23788255",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f9f67a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
